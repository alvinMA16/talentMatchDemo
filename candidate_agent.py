import json
import os
from openai import OpenAI
from helpers import log_model_request, log_model_response, log_processing_step, get_prompt
import re
from datetime import datetime

# Initialize OpenAI client
openai_client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="http://47.93.181.123:4000",
)

class CandidateAgent:
    def __init__(self, resume_info, jd_info, candidate_profile):
        """
        åˆå§‹åŒ–å€™é€‰äººagent
        
        Args:
            resume_info: å®Œæ•´çš„ç®€å†ä¿¡æ¯
            jd_info: è„±æ•çš„èŒä½ä¿¡æ¯
            candidate_profile: å€™é€‰äººæ±‚èŒç”»åƒ
        """
        self.resume_info = resume_info
        self.jd_info = jd_info  # è¿™æ˜¯è„±æ•ç‰ˆæœ¬
        self.candidate_profile = candidate_profile
        self.conversation_history = []  # å­˜å‚¨å®Œæ•´å¯¹è¯å†å²ï¼ˆåŒ…æ‹¬planningï¼‰
        self.chat_history = []  # å­˜å‚¨ç»™å¯¹æ–¹çœ‹çš„chattingå†å²
        self.decision_made = False
        self.final_decision = "UNCERTAIN"
        self.chat_rounds = 0  # è®°å½•å¯¹è¯è½®æ•°
        
    def get_system_prompt(self):
        """è·å–å€™é€‰äººagentçš„ç³»ç»Ÿæç¤º"""
        prompt = get_prompt('candidate_agent_system.txt')
        if not prompt:
            # å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤º
            return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ±‚èŒè€…ä»£ç†ï¼Œä»£è¡¨å€™é€‰äººçš„åˆ©ç›Šè¿›è¡Œæ±‚èŒæ’®åˆã€‚è¯·ç”¨JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«typeã€reasoningã€payloadå­—æ®µã€‚"""
        
        # æ›¿æ¢å€™é€‰äººç”»åƒå ä½ç¬¦
        if self.candidate_profile:
            profile_text = json.dumps(self.candidate_profile, ensure_ascii=False, indent=2)
            prompt = prompt.replace('{CANDIDATE_PROFILE_PLACEHOLDER}', profile_text)
        else:
            prompt = prompt.replace('{CANDIDATE_PROFILE_PLACEHOLDER}', 'æš‚æ— ç”»åƒä¿¡æ¯')
        
        return prompt

    def respond(self, history=None):
        """
        æ ¸å¿ƒæ–¹æ³•ï¼Œæ ¹æ®ç”¨æˆ·æ¶ˆæ¯å’Œå†å²è®°å½•ç”Ÿæˆå›åº”
        """
        system_prompt = get_prompt('candidate_agent_system.txt')
        messages = [{"role": "system", "content": system_prompt}]

        # å¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼Œæ„å»ºåˆå§‹ä¸Šä¸‹æ–‡
        if not history:
            initial_user_prompt = f"""ä»¥ä¸‹æ˜¯ç›¸å…³ä¿¡æ¯ï¼š

å€™é€‰äººç®€å†ä¿¡æ¯ï¼š
{json.dumps(self.resume_info, ensure_ascii=False, indent=2)}

èŒä½æè¿°ä¿¡æ¯ï¼ˆè„±æ•ç‰ˆï¼‰ï¼š
{json.dumps(self.jd_info, ensure_ascii=False, indent=2)}

å€™é€‰äººæ±‚èŒç”»åƒï¼š
{json.dumps(self.candidate_profile, ensure_ascii=False, indent=2)}
"""
            messages.append({"role": "user", "content": initial_user_prompt})
            
            # åˆå§‹å¯åŠ¨æŒ‡ä»¤
            messages.append({"role": "user", "content": "è¯·å¼€å§‹è¯„ä¼°è¿™ä¸ªèŒä½æœºä¼šï¼Œå¹¶ä¸æ‹›è˜æ–¹è¿›è¡Œæ²Ÿé€šã€‚è¯·å…ˆplanningï¼Œç„¶åå‘é€chattingæ¶ˆæ¯ã€‚"})
        else:
            # å¦‚æœæœ‰å†å²è®°å½•ï¼Œç›´æ¥ä½¿ç”¨
            messages.extend(history)
        
        # å¢åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # éµå¾ª LOGGING_GUIDE.md æ ¼å¼è®°å½•æ¨¡å‹è¯·æ±‚
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                model_name = "bedrock-claude-4-sonnet"
                task_type = "CANDIDATE_AGENT_CHAT"
                
                print(f"[{timestamp}] ğŸ¤– MODEL REQUEST | {model_name} | {task_type} | Input: {json.dumps(messages, ensure_ascii=False, indent=2)}")

                log_model_request("bedrock-claude-4-sonnet", "CANDIDATE_AGENT_RESPONSE", 
                                 f"Responding to recruiter message")
                
                response = openai_client.chat.completions.create(
                    model="bedrock-claude-4-sonnet",
                    messages=messages,
                    temperature=1,
                    max_tokens=2000
                )
                
                response_content = response.choices[0].message.content

                # å¢åŠ å¯¹ç©ºå“åº”çš„æ£€æŸ¥å’Œé‡è¯•é€»è¾‘
                if not response_content:
                    if attempt < max_retries - 1:
                        log_processing_step("CANDIDATE_AGENT", "RETRY", f"Empty response on attempt {attempt + 1}, retrying...")
                        continue
                    else:
                        log_model_response("bedrock-claude-4-sonnet", "CANDIDATE_AGENT_RESPONSE", success=False, error_msg="Empty response from model after all retries")
                        return {
                            "type": "chatting",
                            "reasoning": f"æ¨¡å‹åœ¨{max_retries}æ¬¡å°è¯•åä»è¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨é»˜è®¤å›å¤",
                            "payload": "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
                        }

                try:
                    # è§£ææ¨¡å‹è¿”å›çš„JSONå­—ç¬¦ä¸²
                    parsed_response = json.loads(response_content)
                    response_type = parsed_response['type']
                    reasoning = parsed_response['reasoning']
                    payload = parsed_response['payload']
                    
                    if not response_type or not payload:
                        raise ValueError("JSONå“åº”ç¼ºå°‘'type'æˆ–'payload'å­—æ®µ")

                    # è®°å½•å®Œæ•´çš„å¯¹è¯å†å²
                    conversation_entry = {
                        "sender": "candidate",
                        "type": response_type,
                        "reasoning": reasoning,
                        "content": payload,
                        "timestamp": __import__('datetime').datetime.now().isoformat()
                    }
                    self.conversation_history.append(conversation_entry)
                    
                    # å¤„ç†ä¸åŒç±»å‹çš„å›å¤
                    if response_type == "planning":
                        # planningä¸éœ€è¦é¢å¤–å¤„ç†ï¼Œå·²è®°å½•åˆ°å†å²ä¸­
                        pass
                    elif response_type == "chatting":
                        # chattingæ¶ˆæ¯éœ€è¦è®°å½•åˆ°å¯¹æ–¹å¯è§çš„å†å²ä¸­
                        self.chat_rounds += 1
                        self.chat_history.append({
                            "sender": "candidate",
                            "content": payload,
                            "round": self.chat_rounds,
                            "timestamp": conversation_entry["timestamp"]
                        })
                    elif response_type == "decision":
                        # å¤„ç†å†³ç­–
                        self.decision_made = True
                        if payload == "åŒæ„":
                            self.final_decision = "SUITABLE"
                        elif payload == "æ‹’ç»":
                            self.final_decision = "UNSUITABLE"
                        else:
                            self.final_decision = "UNKNOWN"
                    
                    log_model_response("bedrock-claude-4-sonnet", "CANDIDATE_AGENT_RESPONSE", success=True,
                                      output_summary=f"Generated {response_type} message, decision_made: {self.decision_made}")
                    log_processing_step("CANDIDATE_AGENT", "COMPLETE", 
                                       f"Candidate agent generated {response_type} message")
                    
                    return parsed_response
                    
                except json.JSONDecodeError as e:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„ã€å®‰å…¨çš„å›å¤
                    error_message = f"JSONè§£æå¤±è´¥: {e}"
                    log_processing_step("CANDIDATE_AGENT", "DEBUG", f"JSON decode error: {e}")
                    log_model_response("bedrock-claude-4-sonnet", "CANDIDATE_AGENT_RESPONSE", success=False, error_msg=error_message)
                    
                    # è¿”å›ä¸€ä¸ªå®‰å…¨çš„ã€é€šç”¨çš„èŠå¤©æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯è¿”å›Noneæˆ–å´©æºƒ
                    return {
                        "type": "chatting",
                        "reasoning": f"JSONè§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å›å¤",
                        "payload": "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†ä¿¡æ¯æ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    }
                    
            except Exception as e:
                if attempt < max_retries - 1:
                    log_processing_step("CANDIDATE_AGENT", "RETRY", f"Error on attempt {attempt + 1}: {str(e)}, retrying...")
                    continue
                else:
                    # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥äº†
                    log_model_response("bedrock-claude-4-sonnet", "CANDIDATE_AGENT_RESPONSE", success=False, error_msg=f"All {max_retries} attempts failed: {str(e)}")
                    log_processing_step("CANDIDATE_AGENT", "ERROR", f"Error in candidate agent after all retries: {str(e)}")
                    return {
                        "type": "chatting",
                        "reasoning": f"å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å›å¤",
                        "payload": f"æŠ±æ­‰ï¼Œå€™é€‰äººä»£ç†é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼š{str(e)}ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚",
                        "error": True
                    }
    
    def get_conversation_history(self):
        """è·å–å®Œæ•´å¯¹è¯å†å²ï¼ˆåŒ…æ‹¬planningï¼‰"""
        return self.conversation_history
    
    def get_chat_history(self):
        """è·å–å¯¹æ–¹å¯è§çš„chattingå†å²"""
        return self.chat_history
    
    def get_final_decision(self):
        """è·å–æœ€ç»ˆå†³ç­–"""
        return self.final_decision
    
    def has_reached_decision(self):
        """æ£€æŸ¥æ˜¯å¦å·²åšå‡ºæœ€ç»ˆå†³ç­–"""
        return self.decision_made
    
    def get_chat_rounds(self):
        """è·å–å¯¹è¯è½®æ•°"""
        return self.chat_rounds 